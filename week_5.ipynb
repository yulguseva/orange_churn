{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 231) (10000, 230)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"orange_small_churn_train_data.csv\")\n",
    "data_test = pd.read_csv(\"orange_small_churn_test_data.csv\")\n",
    "data.drop(\"ID\", axis = 1, inplace = True)\n",
    "data_test.drop(\"ID\", axis = 1, inplace = True)\n",
    "print data.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = list(data.columns[:190])\n",
    "cat_features = list(data.columns[190:230])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим признаки с большим количеством пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of empty features is 156\n"
     ]
    }
   ],
   "source": [
    "empty_features = []\n",
    "c = 0.3\n",
    "for feat in data.columns:\n",
    "    nulls = data[feat].isnull().value_counts()\n",
    "    try:\n",
    "        not_nulls = nulls[False]\n",
    "        if not_nulls < c*40000:\n",
    "            empty_features.append(feat)\n",
    "    except:    \n",
    "        empty_features.append(feat)\n",
    "print \"number of empty features is\", len(empty_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat in empty_features:\n",
    "    #data.drop(feat, axis = 1, inplace = True)    \n",
    "    if feat in num_features:\n",
    "        num_features.remove(feat)\n",
    "    else:\n",
    "        cat_features.remove(feat)\n",
    "    data.drop(feat,axis=1, inplace=True)\n",
    "    data_test.drop(feat,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 75) (10000, 74)\n"
     ]
    }
   ],
   "source": [
    "print data.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропущенные значения числовых признаков на максимальное значение + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxs = data.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,feat in enumerate(num_features):\n",
    "    fill_value = maxs[i]+1\n",
    "    data.fillna({feat: fill_value}, inplace=True)\n",
    "    data_test.fillna({feat: fill_value}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим значения категориальных признаков на частоты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cat_all = data[cat_features].append(data_test[cat_features])\n",
    "\n",
    "for feat in data_cat_all.columns:\n",
    "    data_cat_all[feat] = data_cat_all[feat].map(data_cat_all.groupby(feat).size())\n",
    "    \n",
    "data[cat_features] = data_cat_all.loc[:39999,:]\n",
    "data_test[cat_features] = data_cat_all.loc[39999:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставшиеся пропущенные значения категориальных признаков заменим нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.fillna(0., inplace=True)\n",
    "data_test.fillna(0., inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 75)\n",
      "(10000, 74)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var38</th>\n",
       "      <th>...</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3052.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>97365.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1662</td>\n",
       "      <td>3</td>\n",
       "      <td>36608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2614</td>\n",
       "      <td>2342</td>\n",
       "      <td>1477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1813.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>166.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2706120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>37009</td>\n",
       "      <td>1</td>\n",
       "      <td>36608.0</td>\n",
       "      <td>11072.0</td>\n",
       "      <td>2108</td>\n",
       "      <td>35156</td>\n",
       "      <td>4354</td>\n",
       "      <td>9804.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1953.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>311.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4698780.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4441</td>\n",
       "      <td>6199</td>\n",
       "      <td>4441</td>\n",
       "      <td>36608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8031</td>\n",
       "      <td>6153</td>\n",
       "      <td>2672</td>\n",
       "      <td>9804.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>220.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>864384.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>37009</td>\n",
       "      <td>34</td>\n",
       "      <td>36608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2108</td>\n",
       "      <td>35156</td>\n",
       "      <td>32703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>686.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>278.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4364880.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>37009</td>\n",
       "      <td>2</td>\n",
       "      <td>36608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4176</td>\n",
       "      <td>35156</td>\n",
       "      <td>32703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Var6  Var7    Var13  Var21  Var22  Var24  Var25   Var28  Var35  \\\n",
       "0  3052.0  36.0  97365.0  480.0  600.0   20.0  480.0  200.00    0.0   \n",
       "1  1813.0   7.0    636.0  212.0  265.0    2.0  128.0  166.56    0.0   \n",
       "2  1953.0   7.0    448.0  176.0  220.0    0.0   72.0  311.76    0.0   \n",
       "3  1533.0   7.0      4.0  332.0  415.0    0.0  144.0  220.08    5.0   \n",
       "4   686.0   7.0      0.0  160.0  200.0    2.0   48.0  278.00    0.0   \n",
       "\n",
       "       Var38   ...    Var220  Var221  Var222   Var223   Var225  Var226  \\\n",
       "0    82752.0   ...         3    1662       3  36608.0      0.0    2614   \n",
       "1  2706120.0   ...         1   37009       1  36608.0  11072.0    2108   \n",
       "2  4698780.0   ...      4441    6199    4441  36608.0      0.0    8031   \n",
       "3   864384.0   ...        34   37009      34  36608.0      0.0    2108   \n",
       "4  4364880.0   ...         2   37009       2  36608.0      0.0    4176   \n",
       "\n",
       "   Var227  Var228  Var229  labels  \n",
       "0    2342    1477     0.0      -1  \n",
       "1   35156    4354  9804.0      -1  \n",
       "2    6153    2672  9804.0      -1  \n",
       "3   35156   32703     0.0       1  \n",
       "4   35156   32703     0.0      -1  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data.shape\n",
    "print data_test.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Размер выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(random_state = 123)\n",
    "X = data.drop(\"labels\", axis = 1)\n",
    "y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sizes = np.arange(0.1,1.1,0.1)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lc = learning_curve(clf, X, y, train_sizes = train_sizes, cv =5, scoring = 'roc_auc', \n",
    "                    shuffle = True, random_state = 12  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for sample of 3199 auc roc on train is 0.960695, on test is 0.706019\n",
      "for sample of 6399 auc roc on train is 0.898029, on test is 0.723727\n",
      "for sample of 9599 auc roc on train is 0.863657, on test is 0.730193\n",
      "for sample of 12799 auc roc on train is 0.842792, on test is 0.736004\n",
      "for sample of 15999 auc roc on train is 0.828983, on test is 0.734760\n",
      "for sample of 19199 auc roc on train is 0.817401, on test is 0.738203\n",
      "for sample of 22399 auc roc on train is 0.809868, on test is 0.739753\n",
      "for sample of 25599 auc roc on train is 0.804365, on test is 0.741986\n",
      "for sample of 28799 auc roc on train is 0.798450, on test is 0.741976\n",
      "for sample of 31999 auc roc on train is 0.794628, on test is 0.744129\n"
     ]
    }
   ],
   "source": [
    "size = lc[0]\n",
    "train_score = lc[1]\n",
    "test_score = lc[2]\n",
    "\n",
    "for s, t1, t2 in zip(size,train_score, test_score):\n",
    "    print 'for sample of %d auc roc on train is %f, on test is %f'%(s, np.mean(t1), np.mean(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на тестовой выборке растет с увеличением выборки. Для лучшего качества стоит ипользовать для обучения 80% выборки и более."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1\\. Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 75)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_class = data[data[\"labels\"] == 1]\n",
    "stay_class = data[data[\"labels\"] == -1]\n",
    "churn_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for weight [1,1] is 0.744002126011\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('labels', axis = 1)\n",
    "y = data['labels']\n",
    "roc_auc = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for weight [1,1] is', np.mean(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42976, 75)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_21 = data.append(churn_class, ignore_index = True)\n",
    "data_21.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for weight [2,1] is 0.770767809306\n"
     ]
    }
   ],
   "source": [
    "X = data_21.drop('labels', axis = 1)\n",
    "y = data_21['labels']\n",
    "roc_auc21 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for weight [2,1] is', np.mean(roc_auc21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48928, 75)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_41 = data_21.append(churn_class, ignore_index = True)\n",
    "data_41 = data_41.append(churn_class, ignore_index = True)\n",
    "data_41.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for weight [4,1] is 0.784580784503\n"
     ]
    }
   ],
   "source": [
    "X = data_41.drop('labels', axis = 1)\n",
    "y = data_41['labels']\n",
    "roc_auc41 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for weight [4,1] is', np.mean(roc_auc41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54880, 75)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_61 = data_41.append(churn_class, ignore_index = True)\n",
    "data_61 = data_61.append(churn_class, ignore_index = True)\n",
    "data_61.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for weight [6,1] is 0.785884211082\n"
     ]
    }
   ],
   "source": [
    "X = data_61.drop('labels', axis = 1)\n",
    "y = data_61['labels']\n",
    "roc_auc61 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for weight [6,1] is', np.mean(roc_auc61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77024, 75)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_12 = data.append(stay_class, ignore_index = True)\n",
    "data_12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for weight [1,2] is 0.748324819344\n"
     ]
    }
   ],
   "source": [
    "X = data_12.drop('labels', axis = 1)\n",
    "y = data_12['labels']\n",
    "roc_auc12 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for weight [1,2] is', np.mean(roc_auc12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data_12, data_21, data_41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение веса класса \"отток\" дает улучшение качества. Лучший результат у веса [6,1] для классов отток и не отток соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2\\. Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 75)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_class = data[data[\"labels\"] == 1]\n",
    "stay_class = data[data[\"labels\"] == -1]\n",
    "churn_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#churn to stay ratio is about 1:12\n",
    "sample_size = len(churn_class)\n",
    "random_indices1 = np.random.choice(stay_class.index, sample_size*3, replace=False)\n",
    "random_indices2 = np.random.choice(stay_class.index, sample_size*3, replace=True)\n",
    "random_indices3 = np.random.choice(stay_class.index, sample_size*2, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for undersampling (replace = False), weight [1,0.25], sample size 11904 is 0.739604\n"
     ]
    }
   ],
   "source": [
    "row_indexer = list(churn_class.index) + list(random_indices1)\n",
    "X = data.iloc[row_indexer, :74]\n",
    "y = data.iloc[row_indexer,74]\n",
    "roc_auc1 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for undersampling (replace = False), weight [1,0.25], sample size %d is %f'%(len(row_indexer),np.mean(roc_auc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for undersampling (replace = True), weight [1,0.25], sample size 11904 is 0.749055\n"
     ]
    }
   ],
   "source": [
    "row_indexer = list(churn_class.index) + list(random_indices2)\n",
    "X = data.iloc[row_indexer, :74]\n",
    "y = data.iloc[row_indexer,74]\n",
    "roc_auc2 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for undersampling (replace = True), weight [1,0.25], sample size %d is %f'%(len(row_indexer),np.mean(roc_auc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for undersampling (replace = True), weight [1,0.16], sample size 8928 is 0.747624\n"
     ]
    }
   ],
   "source": [
    "row_indexer = list(churn_class.index) + list(random_indices3)\n",
    "X = data.iloc[row_indexer, :74]\n",
    "y = data.iloc[row_indexer,74]\n",
    "roc_auc3 = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for undersampling (replace = True), weight [1,0.16], sample size %d is %f'%(len(row_indexer),np.mean(roc_auc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты хуже, чем у оверсемплинга, что неудивительно, так как часть информации остается за бортом. Вариант с возвращениями работает лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка числовых признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Пропущенные значения для числовых признаков обрабатывались так:\n",
    "- заполнялись нулями\n",
    "- заполнялись средними\n",
    "- заполнялись медианами\n",
    "- заполнялись минимальным значением - 1 (чтобы деревья могли отсортировывать пропущенные значения одной вершиной)\n",
    "- заполнялись максимальным значением + 1 (то же)\n",
    "\n",
    "Больших различий обнаружено не было, только выяснилось что заполнять медианами хуже всего, а лучший результат дает последний пункт (но все равно это улучшения порядка нескольких тысячных для auc_roc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 231)\n"
     ]
    }
   ],
   "source": [
    "data3 = pd.read_csv(\"orange_small_churn_train_data.csv\")\n",
    "data3.drop(\"ID\", axis = 1, inplace = True)\n",
    "print data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 75)\n"
     ]
    }
   ],
   "source": [
    "for feat in empty_features:\n",
    "    data3.drop(feat,axis=1, inplace=True)\n",
    "print data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mins = data3.min(axis = 0)\n",
    "maxs = data3.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data30 = data3.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, feat in enumerate(num_features):\n",
    "    m1 = np.mean(data3[feat].dropna())\n",
    "    m2 = np.median(data3[feat].dropna())\n",
    "    m3 = mins[i]-1\n",
    "    m4 = maxs[i]+1\n",
    "    m_list = [m1, m2, m3, m4]\n",
    "    #print feat, m_list\n",
    "    if i == 0:\n",
    "        data31 = data3.fillna({feat: m1})\n",
    "        data32 = data3.fillna({feat: m2})\n",
    "        data33 = data3.fillna({feat: m3})\n",
    "        data34 = data3.fillna({feat: m4})\n",
    "        data_list = [data31, data32, data33, data34]\n",
    "    else: \n",
    "        for d, m in zip(data_list, m_list):\n",
    "            d = d.fillna({feat:m}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for zeros filling roc_auc is 0.742215\n",
      "for means filling roc_auc is 0.742754\n",
      "for meds filling roc_auc is 0.734380\n",
      "for min - 1 filling roc_auc is 0.740390\n",
      "for max + 1 filling roc_auc is 0.744002\n"
     ]
    }
   ],
   "source": [
    "data_list = [data30, data31, data32, data33, data34]\n",
    "descr_list = ['zeros', 'means', 'meds', 'min - 1', 'max + 1']\n",
    "roc_auc_list = []\n",
    "for d, descr in zip(data_list, descr_list):\n",
    "    X = d[num_features].join(data[cat_features])\n",
    "    y = d['labels']\n",
    "    ra = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "    roc_auc_list.append(ra)\n",
    "    print 'for %s filling roc_auc is %f'%(descr, np.mean(ra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка категоральных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Категориальные признаки обрабатывались так:\n",
    "- хэширование признаков\n",
    "- OneHotEncoding (для признаков с <400 значений) + хэширование (остальных)\n",
    "- заполнение частотой данного значени\n",
    "\n",
    "Лучший результат показал последний вариант. Второй вариант неудобен тем, что добавляет большое количество новых признаков, что замедляет работу бустинга, и вообще он больше подходит для регрессий, чем для деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 231)\n",
      "(40000, 75)\n"
     ]
    }
   ],
   "source": [
    "data4 = pd.read_csv(\"orange_small_churn_train_data.csv\")\n",
    "data4.drop(\"ID\", axis = 1, inplace = True)\n",
    "print data4.shape\n",
    "for feat in empty_features:\n",
    "    data4.drop(feat,axis=1, inplace=True)\n",
    "print data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4[num_features] = data[num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "hash_space = 400\n",
    "\n",
    "cat_x_hashed = pd.DataFrame()\n",
    "data_set = [data4]\n",
    "hash_set = [cat_x_hashed]\n",
    "\n",
    "for feat in cat_features:\n",
    "    for d, h in zip(data_set, hash_set):\n",
    "        feat_hashed = [hash(x) % hash_space for x in d[feat]]\n",
    "        h[str(feat)] = pd.Series(feat_hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var192</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var196</th>\n",
       "      <th>Var197</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var200</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var203</th>\n",
       "      <th>...</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>61</td>\n",
       "      <td>366</td>\n",
       "      <td>320</td>\n",
       "      <td>266</td>\n",
       "      <td>147</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>225</td>\n",
       "      <td>332</td>\n",
       "      <td>132</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>313</td>\n",
       "      <td>184</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240</td>\n",
       "      <td>249</td>\n",
       "      <td>366</td>\n",
       "      <td>320</td>\n",
       "      <td>348</td>\n",
       "      <td>10</td>\n",
       "      <td>111</td>\n",
       "      <td>33</td>\n",
       "      <td>220</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>34</td>\n",
       "      <td>189</td>\n",
       "      <td>101</td>\n",
       "      <td>206</td>\n",
       "      <td>368</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113</td>\n",
       "      <td>98</td>\n",
       "      <td>366</td>\n",
       "      <td>320</td>\n",
       "      <td>192</td>\n",
       "      <td>151</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>71</td>\n",
       "      <td>102</td>\n",
       "      <td>248</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>237</td>\n",
       "      <td>314</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>366</td>\n",
       "      <td>320</td>\n",
       "      <td>387</td>\n",
       "      <td>58</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>237</td>\n",
       "      <td>189</td>\n",
       "      <td>216</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198</td>\n",
       "      <td>120</td>\n",
       "      <td>366</td>\n",
       "      <td>320</td>\n",
       "      <td>301</td>\n",
       "      <td>76</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>289</td>\n",
       "      <td>189</td>\n",
       "      <td>226</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var192  Var193  Var195  Var196  Var197  Var198  Var199  Var200  Var202  \\\n",
       "0     175      61     366     320     266     147     357       0     191   \n",
       "1     240     249     366     320     348      10     111      33     220   \n",
       "2     113      98     366     320     192     151      44       0       3   \n",
       "3      67     120     366     320     387      58     252       0      99   \n",
       "4     198     120     366     320     301      76     353       0     257   \n",
       "\n",
       "   Var203   ...    Var219  Var220  Var221  Var222  Var223  Var225  Var226  \\\n",
       "0     154   ...       167     225     332     132     206       0     313   \n",
       "1     154   ...       167      34     189     101     206     368     277   \n",
       "2     154   ...       167      71     102     248     206       0     304   \n",
       "3     156   ...       167     237     189     216     206       0     277   \n",
       "4     154   ...       167     289     189     226     206       0     128   \n",
       "\n",
       "   Var227  Var228  Var229  \n",
       "0     184      60       0  \n",
       "1       0     386     207  \n",
       "2     237     314     207  \n",
       "3       0     113       0  \n",
       "4       0     113       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_x_hashed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже используется отбор признаков по количеству пропущенных значений (выброшены те, у которых количество пропусков более 80%). Это несильно улучшает результат, зато помогает быстрее обсчитывать модели. Попробуем дальнейший отбор признаков по дисперсии значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('labels', axis = 1)\n",
    "y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for 64 selected features via VarieanceThreshold is 0.743889\n"
     ]
    }
   ],
   "source": [
    "#roc_auc without variance selection is 0.744002126011\n",
    "selector = VarianceThreshold(500.0)\n",
    "X_sel = selector.fit_transform(X)\n",
    "roc_auc = cross_val_score(clf, X_sel, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for %d selected features via VarieanceThreshold is %f'% (X_sel.shape[1], np.mean(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for 62 selected features via VarieanceThreshold is 0.745222\n"
     ]
    }
   ],
   "source": [
    "selector = VarianceThreshold(1000.0)\n",
    "X_sel = selector.fit_transform(X)\n",
    "roc_auc = cross_val_score(clf, X_sel, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for %d selected features via VarieanceThreshold is %f'% (X_sel.shape[1], np.mean(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc for 60 selected features via VarieanceThreshold is 0.697783\n"
     ]
    }
   ],
   "source": [
    "selector = VarianceThreshold(1500.0)\n",
    "X_sel = selector.fit_transform(X)\n",
    "roc_auc = cross_val_score(clf, X_sel, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc for %d selected features via VarieanceThreshold is %f'% (X_sel.shape[1], np.mean(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, отбор признаков по дисперсии для значений 500 и 1000 значимого влияния на результат не оказывает, а при значении 1500 качество падает -- мы начинаем отсеивать важные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для поиска параметров по сетке будем использовать лучшую на данный момент конфигурацию: балансировка с весами [6,1], заполнение пропущенных значений вещественных признаков на max + 1, кодирование категориальных признаков частотами, удалений пустых и почти пустых (>80% Nans) признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43905, 74) (10975, 74)\n"
     ]
    }
   ],
   "source": [
    "X = data_61.drop('labels', axis = 1)\n",
    "y = data_61['labels']\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "a = skf.split(X, y)\n",
    "for train_index, test_index in skf.split(X, y):    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=123, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [50, 100, 200, 400], 'learning_rate': [0.1, 0.03], 'max_depth': [3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GradientBoostingClassifier(random_state = 123)\n",
    "\n",
    "params = {'n_estimators' : [50, 100, 200, 400],\n",
    "    'max_depth' : [3,4], \n",
    "    'learning_rate' : [0.1, 0.03]\n",
    "         }\n",
    "gs1 = GridSearchCV(clf, params, cv = 5, scoring = 'roc_auc')\n",
    "gs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91991957633\n",
      "{'n_estimators': 400, 'learning_rate': 0.1, 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "print gs1.best_score_\n",
    "print gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.419707</td>\n",
       "      <td>0.031318</td>\n",
       "      <td>0.766616</td>\n",
       "      <td>0.772149</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'n_estimators': 50, u'learning_rate': 0.1, u...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.766545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765806</td>\n",
       "      <td>0.772359</td>\n",
       "      <td>0.768223</td>\n",
       "      <td>0.772408</td>\n",
       "      <td>0.766347</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.312612</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.925349</td>\n",
       "      <td>0.043650</td>\n",
       "      <td>0.787367</td>\n",
       "      <td>0.796412</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'n_estimators': 100, u'learning_rate': 0.1, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.788340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787694</td>\n",
       "      <td>0.796503</td>\n",
       "      <td>0.786925</td>\n",
       "      <td>0.795103</td>\n",
       "      <td>0.788316</td>\n",
       "      <td>0.797748</td>\n",
       "      <td>0.166803</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.413946</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.815060</td>\n",
       "      <td>0.829637</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'n_estimators': 200, u'learning_rate': 0.1, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.816816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816156</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>0.814510</td>\n",
       "      <td>0.829159</td>\n",
       "      <td>0.815148</td>\n",
       "      <td>0.829169</td>\n",
       "      <td>1.815741</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.976458</td>\n",
       "      <td>0.139926</td>\n",
       "      <td>0.854798</td>\n",
       "      <td>0.876918</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>{u'n_estimators': 400, u'learning_rate': 0.1, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855523</td>\n",
       "      <td>0.876754</td>\n",
       "      <td>0.853544</td>\n",
       "      <td>0.875761</td>\n",
       "      <td>0.855393</td>\n",
       "      <td>0.877050</td>\n",
       "      <td>9.854533</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.867842</td>\n",
       "      <td>0.043753</td>\n",
       "      <td>0.791351</td>\n",
       "      <td>0.801742</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'n_estimators': 50, u'learning_rate': 0.1, u...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.792009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791861</td>\n",
       "      <td>0.803931</td>\n",
       "      <td>0.792511</td>\n",
       "      <td>0.801421</td>\n",
       "      <td>0.791621</td>\n",
       "      <td>0.802173</td>\n",
       "      <td>1.608810</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34.789507</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>0.822030</td>\n",
       "      <td>0.837920</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'n_estimators': 100, u'learning_rate': 0.1, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.823516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820693</td>\n",
       "      <td>0.837320</td>\n",
       "      <td>0.822295</td>\n",
       "      <td>0.837259</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.838991</td>\n",
       "      <td>0.398701</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>67.605288</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>0.865966</td>\n",
       "      <td>0.889225</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'n_estimators': 200, u'learning_rate': 0.1, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.870038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865091</td>\n",
       "      <td>0.888291</td>\n",
       "      <td>0.864389</td>\n",
       "      <td>0.888055</td>\n",
       "      <td>0.866385</td>\n",
       "      <td>0.890194</td>\n",
       "      <td>0.345275</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>133.357233</td>\n",
       "      <td>0.144926</td>\n",
       "      <td>0.919920</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>{u'n_estimators': 400, u'learning_rate': 0.1, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919577</td>\n",
       "      <td>0.947693</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.948846</td>\n",
       "      <td>0.919539</td>\n",
       "      <td>0.947703</td>\n",
       "      <td>0.751673</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.577536</td>\n",
       "      <td>0.030798</td>\n",
       "      <td>0.741621</td>\n",
       "      <td>0.744680</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'n_estimators': 50, u'learning_rate': 0.03, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.742775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740761</td>\n",
       "      <td>0.744243</td>\n",
       "      <td>0.742219</td>\n",
       "      <td>0.744061</td>\n",
       "      <td>0.744051</td>\n",
       "      <td>0.744652</td>\n",
       "      <td>0.036626</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.437614</td>\n",
       "      <td>0.044219</td>\n",
       "      <td>0.755673</td>\n",
       "      <td>0.760057</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'n_estimators': 100, u'learning_rate': 0.03,...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.756873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754085</td>\n",
       "      <td>0.759787</td>\n",
       "      <td>0.756173</td>\n",
       "      <td>0.759868</td>\n",
       "      <td>0.757803</td>\n",
       "      <td>0.760379</td>\n",
       "      <td>0.082004</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>43.880592</td>\n",
       "      <td>0.070793</td>\n",
       "      <td>0.771327</td>\n",
       "      <td>0.777565</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'n_estimators': 200, u'learning_rate': 0.03,...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769381</td>\n",
       "      <td>0.776481</td>\n",
       "      <td>0.771653</td>\n",
       "      <td>0.777381</td>\n",
       "      <td>0.772566</td>\n",
       "      <td>0.777657</td>\n",
       "      <td>0.297796</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>85.163968</td>\n",
       "      <td>0.121194</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.803815</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>{u'n_estimators': 400, u'learning_rate': 0.03,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.794774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794098</td>\n",
       "      <td>0.804627</td>\n",
       "      <td>0.793231</td>\n",
       "      <td>0.802715</td>\n",
       "      <td>0.795209</td>\n",
       "      <td>0.804007</td>\n",
       "      <td>0.264420</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.000660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18.983953</td>\n",
       "      <td>0.035089</td>\n",
       "      <td>0.758047</td>\n",
       "      <td>0.763223</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'n_estimators': 50, u'learning_rate': 0.03, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.758075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758231</td>\n",
       "      <td>0.763734</td>\n",
       "      <td>0.760620</td>\n",
       "      <td>0.762902</td>\n",
       "      <td>0.758384</td>\n",
       "      <td>0.762834</td>\n",
       "      <td>0.152838</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.424448</td>\n",
       "      <td>0.055148</td>\n",
       "      <td>0.774797</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'n_estimators': 100, u'learning_rate': 0.03,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.774829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773848</td>\n",
       "      <td>0.782728</td>\n",
       "      <td>0.775551</td>\n",
       "      <td>0.780931</td>\n",
       "      <td>0.775915</td>\n",
       "      <td>0.782868</td>\n",
       "      <td>0.143201</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70.956597</td>\n",
       "      <td>0.085772</td>\n",
       "      <td>0.799230</td>\n",
       "      <td>0.810413</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'n_estimators': 200, u'learning_rate': 0.03,...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.799037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798220</td>\n",
       "      <td>0.811140</td>\n",
       "      <td>0.800885</td>\n",
       "      <td>0.809903</td>\n",
       "      <td>0.799611</td>\n",
       "      <td>0.810842</td>\n",
       "      <td>0.395410</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>136.702184</td>\n",
       "      <td>0.151787</td>\n",
       "      <td>0.832148</td>\n",
       "      <td>0.849555</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>{u'n_estimators': 400, u'learning_rate': 0.03,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.831826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832098</td>\n",
       "      <td>0.850223</td>\n",
       "      <td>0.833554</td>\n",
       "      <td>0.849048</td>\n",
       "      <td>0.833325</td>\n",
       "      <td>0.850136</td>\n",
       "      <td>0.523040</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       11.419707         0.031318         0.766616          0.772149   \n",
       "1       21.925349         0.043650         0.787367          0.796412   \n",
       "2       45.413946         0.074100         0.815060          0.829637   \n",
       "3      101.976458         0.139926         0.854798          0.876918   \n",
       "4       23.867842         0.043753         0.791351          0.801742   \n",
       "5       34.789507         0.050793         0.822030          0.837920   \n",
       "6       67.605288         0.081928         0.865966          0.889225   \n",
       "7      133.357233         0.144926         0.919920          0.948371   \n",
       "8       11.577536         0.030798         0.741621          0.744680   \n",
       "9       22.437614         0.044219         0.755673          0.760057   \n",
       "10      43.880592         0.070793         0.771327          0.777565   \n",
       "11      85.163968         0.121194         0.793803          0.803815   \n",
       "12      18.983953         0.035089         0.758047          0.763223   \n",
       "13      36.424448         0.055148         0.774797          0.782123   \n",
       "14      70.956597         0.085772         0.799230          0.810413   \n",
       "15     136.702184         0.151787         0.832148          0.849555   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "0                  0.1               3                 50   \n",
       "1                  0.1               3                100   \n",
       "2                  0.1               3                200   \n",
       "3                  0.1               3                400   \n",
       "4                  0.1               4                 50   \n",
       "5                  0.1               4                100   \n",
       "6                  0.1               4                200   \n",
       "7                  0.1               4                400   \n",
       "8                 0.03               3                 50   \n",
       "9                 0.03               3                100   \n",
       "10                0.03               3                200   \n",
       "11                0.03               3                400   \n",
       "12                0.03               4                 50   \n",
       "13                0.03               4                100   \n",
       "14                0.03               4                200   \n",
       "15                0.03               4                400   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {u'n_estimators': 50, u'learning_rate': 0.1, u...               13   \n",
       "1   {u'n_estimators': 100, u'learning_rate': 0.1, ...               10   \n",
       "2   {u'n_estimators': 200, u'learning_rate': 0.1, ...                6   \n",
       "3   {u'n_estimators': 400, u'learning_rate': 0.1, ...                3   \n",
       "4   {u'n_estimators': 50, u'learning_rate': 0.1, u...                9   \n",
       "5   {u'n_estimators': 100, u'learning_rate': 0.1, ...                5   \n",
       "6   {u'n_estimators': 200, u'learning_rate': 0.1, ...                2   \n",
       "7   {u'n_estimators': 400, u'learning_rate': 0.1, ...                1   \n",
       "8   {u'n_estimators': 50, u'learning_rate': 0.03, ...               16   \n",
       "9   {u'n_estimators': 100, u'learning_rate': 0.03,...               15   \n",
       "10  {u'n_estimators': 200, u'learning_rate': 0.03,...               12   \n",
       "11  {u'n_estimators': 400, u'learning_rate': 0.03,...                8   \n",
       "12  {u'n_estimators': 50, u'learning_rate': 0.03, ...               14   \n",
       "13  {u'n_estimators': 100, u'learning_rate': 0.03,...               11   \n",
       "14  {u'n_estimators': 200, u'learning_rate': 0.03,...                7   \n",
       "15  {u'n_estimators': 400, u'learning_rate': 0.03,...                4   \n",
       "\n",
       "    split0_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "0            0.766545       ...                  0.765806            0.772359   \n",
       "1            0.788340       ...                  0.787694            0.796503   \n",
       "2            0.816816       ...                  0.816156            0.829714   \n",
       "3            0.856776       ...                  0.855523            0.876754   \n",
       "4            0.792009       ...                  0.791861            0.803931   \n",
       "5            0.823516       ...                  0.820693            0.837320   \n",
       "6            0.870038       ...                  0.865091            0.888291   \n",
       "7            0.921626       ...                  0.919577            0.947693   \n",
       "8            0.742775       ...                  0.740761            0.744243   \n",
       "9            0.756873       ...                  0.754085            0.759787   \n",
       "10           0.773438       ...                  0.769381            0.776481   \n",
       "11           0.794774       ...                  0.794098            0.804627   \n",
       "12           0.758075       ...                  0.758231            0.763734   \n",
       "13           0.774829       ...                  0.773848            0.782728   \n",
       "14           0.799037       ...                  0.798220            0.811140   \n",
       "15           0.831826       ...                  0.832098            0.850223   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0            0.768223            0.772408           0.766347   \n",
       "1            0.786925            0.795103           0.788316   \n",
       "2            0.814510            0.829159           0.815148   \n",
       "3            0.853544            0.875761           0.855393   \n",
       "4            0.792511            0.801421           0.791621   \n",
       "5            0.822295            0.837259           0.823300   \n",
       "6            0.864389            0.888055           0.866385   \n",
       "7            0.921260            0.948846           0.919539   \n",
       "8            0.742219            0.744061           0.744051   \n",
       "9            0.756173            0.759868           0.757803   \n",
       "10           0.771653            0.777381           0.772566   \n",
       "11           0.793231            0.802715           0.795209   \n",
       "12           0.760620            0.762902           0.758384   \n",
       "13           0.775551            0.780931           0.775915   \n",
       "14           0.800885            0.809903           0.799611   \n",
       "15           0.833554            0.849048           0.833325   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.771028      0.312612        0.001465        0.000839   \n",
       "1             0.797748      0.166803        0.001782        0.001041   \n",
       "2             0.829169      1.815741        0.003565        0.001435   \n",
       "3             0.877050      9.854533        0.008013        0.001452   \n",
       "4             0.802173      1.608810        0.005391        0.001331   \n",
       "5             0.838991      0.398701        0.001930        0.001305   \n",
       "6             0.890194      0.345275        0.002008        0.002199   \n",
       "7             0.947703      0.751673        0.002791        0.001440   \n",
       "8             0.744652      0.036626        0.000133        0.001968   \n",
       "9             0.760379      0.082004        0.000205        0.001659   \n",
       "10            0.777657      0.297796        0.001869        0.001604   \n",
       "11            0.804007      0.264420        0.001722        0.001244   \n",
       "12            0.762834      0.152838        0.000192        0.001818   \n",
       "13            0.782868      0.143201        0.001590        0.000852   \n",
       "14            0.810842      0.395410        0.001115        0.000963   \n",
       "15            0.850136      0.523040        0.002811        0.001293   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.000717  \n",
       "1          0.000839  \n",
       "2          0.000507  \n",
       "3          0.000744  \n",
       "4          0.001244  \n",
       "5          0.000780  \n",
       "6          0.001502  \n",
       "7          0.001285  \n",
       "8          0.000555  \n",
       "9          0.000213  \n",
       "10         0.000796  \n",
       "11         0.000660  \n",
       "12         0.000329  \n",
       "13         0.000779  \n",
       "14         0.000742  \n",
       "15         0.000515  \n",
       "\n",
       "[16 rows x 23 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(gs1.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43905, 74) (10975, 74)\n"
     ]
    }
   ],
   "source": [
    "X = data_61.drop('labels', axis = 1)\n",
    "y = data_61['labels']\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "a = skf.split(X, y)\n",
    "for train_index, test_index in skf.split(X, y):    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91020823093044534"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 400, learning_rate = 0.1, max_depth = 4, random_state = 123)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test) \n",
    "y_prob = [x[1] for x in y_pred]\n",
    "roc_auc_ho = roc_auc_score(y_test, y_prob)\n",
    "roc_auc_ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.061246</td>\n",
       "      <td>Var113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.060648</td>\n",
       "      <td>Var57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.045030</td>\n",
       "      <td>Var126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.036044</td>\n",
       "      <td>Var192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.034103</td>\n",
       "      <td>Var199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029631</td>\n",
       "      <td>Var6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.028810</td>\n",
       "      <td>Var125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.028711</td>\n",
       "      <td>Var189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.027933</td>\n",
       "      <td>Var94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.027515</td>\n",
       "      <td>Var81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    importance feature\n",
       "24    0.061246  Var113\n",
       "11    0.060648   Var57\n",
       "28    0.045030  Var126\n",
       "42    0.036044  Var192\n",
       "48    0.034103  Var199\n",
       "0     0.029631    Var6\n",
       "27    0.028810  Var125\n",
       "41    0.028711  Var189\n",
       "21    0.027933   Var94\n",
       "18    0.027515   Var81"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_importance = pd.DataFrame(clf.feature_importances_, columns = ['importance'])\n",
    "f_importance['feature'] = X.columns\n",
    "f_importance.sort_values(by=['importance'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features with importance > 0 is 74\n",
      "# of features with importance > 0.005 is 48\n"
     ]
    }
   ],
   "source": [
    "print '# of features with importance > 0 is', f_importance[f_importance.importance > 0].shape[0]\n",
    "print '# of features with importance > 0.005 is', f_importance[f_importance.importance > 0.005].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = pd.DataFrame(y_test)\n",
    "diff['pred'] = y_prob\n",
    "y_pred = clf.predict(X_test)\n",
    "diff['class'] = y_pred\n",
    "diff = diff[diff['labels'] != diff['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kustarev/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "diff['error'] = np.where(diff['class'] == 1, diff.pred, 1-diff.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>pred</th>\n",
       "      <th>class</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40332</th>\n",
       "      <td>1</td>\n",
       "      <td>0.053852</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.946148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43308</th>\n",
       "      <td>1</td>\n",
       "      <td>0.053852</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.946148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46205</th>\n",
       "      <td>1</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.937780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45167</th>\n",
       "      <td>1</td>\n",
       "      <td>0.069607</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.930393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>1</td>\n",
       "      <td>0.072159</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.927841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51263</th>\n",
       "      <td>1</td>\n",
       "      <td>0.079509</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.920491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40313</th>\n",
       "      <td>1</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.914897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46265</th>\n",
       "      <td>1</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.914897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50467</th>\n",
       "      <td>1</td>\n",
       "      <td>0.086406</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.913594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41539</th>\n",
       "      <td>1</td>\n",
       "      <td>0.086406</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.913594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels      pred  class     error\n",
       "40332       1  0.053852     -1  0.946148\n",
       "43308       1  0.053852     -1  0.946148\n",
       "46205       1  0.062220     -1  0.937780\n",
       "45167       1  0.069607     -1  0.930393\n",
       "53071       1  0.072159     -1  0.927841\n",
       "51263       1  0.079509     -1  0.920491\n",
       "40313       1  0.085103     -1  0.914897\n",
       "46265       1  0.085103     -1  0.914897\n",
       "50467       1  0.086406     -1  0.913594\n",
       "41539       1  0.086406     -1  0.913594"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.sort_values(by=['error'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inq = diff.sort_values(by=['error'], ascending = False)\n",
    "inq = inq[inq.index < 40000]\n",
    "inq_index = inq.sort_values(by=['error'], ascending = False).index\n",
    "d = data.iloc[inq_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для поиска параметров по сетке будем использовать лучшую на данный момент конфигурацию: балансировка с весами [6,1], заполнение пропущенных значений вещественных признаков на max + 1, кодирование категориальных признаков частотами, удалений пустых и почти пустых (>80% Nans) признаков. Параметры берем из research grid n_estimators = 400, learning_rate = 0.1, max_depth = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 400, learning_rate = 0.1, max_depth = 4, random_state = 123)\n",
    "#clf = GradientBoostingClassifier(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91991957633\n"
     ]
    }
   ],
   "source": [
    "X = data_61.drop('labels', axis = 1)\n",
    "y = data_61['labels']\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "a = skf.split(X, y)\n",
    "for train_index, test_index in skf.split(X, y):    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91020823093044534"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test) \n",
    "y_prob = [x[1] for x in y_pred]\n",
    "roc_auc_ho = roc_auc_score(y_test, y_prob)\n",
    "#0.78026723279290344 with var selection\n",
    "roc_auc_ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc best model in cv is 0.732266\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(\"labels\", axis = 1)\n",
    "y = data['labels']\n",
    "\n",
    "roc_auc = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "print 'roc auc best model in cv is %f'% (np.mean(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### данное решение без оверсемплинга набрало на kaggle pub score 0.70311, pr score 0.73653\n",
    "### с оверсемплингом [6,1] 0.72923, 0.68874\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать другие классификаторы (например, ридж регрессию) и построить ансамбль из них."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
